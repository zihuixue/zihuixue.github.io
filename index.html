<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta name="google-site-verification" content="ufZa3XjmGskngtrL5yh3aMO6n4-QlFMFAyP2kxPsmRw" />
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zihui Xue</title>

  <meta name="author" content="Zihui Xue">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BQ6J8EFY6Z"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BQ6J8EFY6Z');
</script>
</head>

<body>
  <table
    style="width:110%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zihui (Sherry) Xue </name>
                  </p>
                  <br>
                  <p>
                    Hi, I am Zihui Xue (ËñõÂ≠êÊÖß), a Ph.D. student at UT Austin, advised by <a href="https://www.cs.utexas.edu/users/grauman/">Prof. Kristen Grauman</a>. I am also a visiting researcher at <a href="https://ai.facebook.com">FAIR, Meta AI</a>.

                    Previously, I'm fortunate to work with <a href="https://radum.ece.utexas.edu">Prof. Radu Marculescu</a> on efficient deep learning and <a href="https://hangzhaomit.github.io">Prof. Hang Zhao</a> on multimodal learning. I obtained my bachelor's degree from Fudan University in 2020.
                    <!-- where I worked with <a href="http://medianet.azurewebsites.net">Prof. Yuedong Xu </a>. -->
                  </p>
                  <p>
                    My research interests lie in egocentric video understanding and multimodal learning.
                  </p>

                  <p style="text-align:center">
                    <a href="data/email.txt">Email</a> &nbsp|&nbsp
                    <a href="data/CV_Zihui Xue.pdf">CV</a> &nbsp|&nbsp
                    <a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&authuser=2&gmla=AJsN-F49ZRjKgLxe5u7PyuIHAq0QUMrAApYmolkUVk6TjuP8V8AEI60VMC-x2U7-6Ey6R5usFH881WFZkTNJohdyfgsW7TQMx-rBNf4fWjpPiMThSuXU8G0&user=JCV9BQ0AAAAJ">Google Scholar</a> &nbsp|&nbsp
                    <a href="https://github.com/zihuixue">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <div>
                  <a href="images/profile4.jpeg"><img style="width:80%;max-width:80%" alt="profile photo"
                      src="images/profile4.jpeg" class="hoverZoomLink"></a>
                      <!-- <span> <center>Photo taken by <a href="https://zhengqigao.github.io">Zhengqi Gao </a> </center> </span> -->
                    </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="110%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="110%" align="center" border="0" cellpadding="20">
            <tbody>
              <ul>
                <news>
                <li>[Feb. 2023] <a href="https://vision.cs.utexas.edu/projects/egot2/">EgoT2</a> got accepted by CVPR'23 as Highlight. See you in Vancouver!</li>
                <li>[Jan. 2023] <a href="https://zihuixue.github.io/MFH/index.html">MFH</a> got accepted by ICLR'23 (top-5%).</li>
                <li>[Aug. 2022] Spent a wonderful summer interning at FAIR, Meta AI, working with <a href="https://ltorresa.github.io/home.html">Lorenzo Torresani </a> üòä  </li>
                <!-- <li>[Jan. 2022] <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Co-Advise_Cross_Inductive_Bias_Distillation_CVPR_2022_paper.pdf">Co-advise</a> got accepted by CVPR'22 üéâ </li> -->
                <!-- <li>[Jan. 2022] Check out our <a href="https://arxiv.org/abs/2202.00075"> SUGAR paper </a> on efficient GNN training üôá</li> -->
                <li>[Sep. 2021] One paper got accepted by NeurIPS'21.</li>
                <li>[Sep. 2021] One paper got accepted by CoRL'21.</li>
                <li>[Jul. 2021] Two papers got accepted by ICCV'21.</li>
                <li>[Aug. 2020] Start working with <a href='http://people.csail.mit.edu/hangzhao/'> Prof. Hang Zhao </a> at Shanghai Qi Zhi Institue, Tsinghua University on multimodal learning üòä </li>
                </news>
              </ul>
            </tbody>
          </table>



          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <heading>Projects</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <subheading>Egocentric Video Understanding</subheading>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/egot2.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>Egocentric Video Task Translation
                    </papertitle>
                  <br>
                  <br>
                  <paper>
                  <b>Zihui Xue</b>,
                  Yale Song,
                  Kristen Grauman,
                  Lorenzo Torresani
                  <br>
                  </paper>
                  <papervenue> CVPR Highlight, 2023 (top-2.5%)</papervenue>
                  <a href="https://arxiv.org/abs/2212.06301">[paper]</a>
                  <a href="https://vision.cs.utexas.edu/projects/egot2/">[webpage]</a>
                  <br>
                  <paper>Hollistic egocentric perception for a set of diverse video tasks</paper>
                </td>
              </tr>
          </table>

           <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <subheading>Multimodal Learning and Self-supervised Learning</subheading>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/modvenn.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation
                    </papertitle>
                  <br>
                  <br>
                  <paper>
                  <b>Zihui Xue*</b>,
                  Zhengqi Gao*
                  Sucheng Ren*,
                  Hang Zhao
                  <br>
                  </paper>
                  <papervenue> ICLR, 2023 (top-5%)</papervenue>
                  <a href="https://openreview.net/forum?id=w0QXrZ3N-s">[paper]</a>
                  <a href="MFH/index.html">[webpage]</a>
                  <br>
                  <paper>When is crossmodal knowledge distillation helpful?</paper>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/dynmm.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>Dynamic Multimodal Fusion
                    </papertitle>
                  <br>
                  <br>
                  <paper>
                  <b>Zihui Xue</b>,
                  Radu Marculescu
                  <br>
                </paper>
                  <papervenue> CVPR MULA workshop, 2023</papervenue>
                  <a href="https://arxiv.org/abs/2204.00102">[paper]</a>
                  <br>
                <paper>Adaptively fuse multimodal data and generate data-dependent forward paths during inference time.</paper>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/mmbetter.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>What Makes Multi-Modal Learning Better than Single (Provably)
                    </papertitle>
                  <br>
                  <br>
                <paper>
                  Yu Huang,
                  Chenzhuang Du,
                  <b>Zihui Xue</b>,
                  Xuanyao Chen,
                  Hang Zhao,
                  Longbo Huang
                </paper>
                  <br>
                  <papervenue>NeurIPS, 2021</papervenue>
                  <a href="https://openreview.net/forum?id=UlSjqPEkI1V">[paper]</a>
                  <br>
                  <paper>
                    Can multimodal learning provably perform better than unimodal?
                  </paper>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/MKE.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>Multimodal Knowledge Expansion
                    </papertitle>
                  <br>
                  <br>
                <paper>
                  <b>Zihui Xue</b>,
                  Sucheng Ren,
                  Zhengqi Gao,
                  Hang Zhao
                </paper>
                  <br>
                  <papervenue>
                  ICCV, 2021
                  </papervenue>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xue_Multimodal_Knowledge_Expansion_ICCV_2021_paper.pdf">[paper]</a>
                  <a href="https://tsinghua-mars-lab.github.io/MKE/">[webpage]</a>
                  <br>
                <paper>
                    A knowledge distillation-based framework to effectively utilize multimodal data without requiring labels.
                  </paper>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/decorr.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>On Feature Decorrelation in Self-Supervised Learning
                    </papertitle>
                  <br>
                  <br>
                <paper>
                  Tianyu Hua,
                  Wenxiao Wang,
                  <b>Zihui Xue</b>,
                  Sucheng Ren,
                  Yue Wang,
                  Hang Zhao
                </paper>

                  <br>
                  <papervenue>ICCV, 2021 (Oral, Acceptance Rate 3.0%)</papervenue>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hua_On_Feature_Decorrelation_in_Self-Supervised_Learning_ICCV_2021_paper.pdf">[paper]</a>
                  <a href="https://tsinghua-mars-lab.github.io/decorr/">[webpage]</a>
                  <br>
                <paper>
                    Reveal the connection between model collapse and feature correlations!
                  </paper>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <subheading>Efficient Deep Learning</subheading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/sugar.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>SUGAR: Efficient Subgraph-level Training via Resource-aware Graph Partitioning
                    </papertitle>
                  <br>
                  <br>
                <paper>
                  <b>Zihui Xue</b>,
                  Yuedong Yang,
                  Mengtian Yang,
                  Radu Marculescu
                </paper>
                  <br>
                  <papervenue>
                    IEEE Transactions on Computers, 2023
                  </papervenue>
                  <a href="https://arxiv.org/abs/2202.00075">[paper]</a>
                  <br>
                  <paper>
                    An efficient GNN training framework that accounts for resource constraints.
                </paper>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:40%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/depth.png"></div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                    <papertitle>Anytime Depth Estimation with Limited Sensing and Computation Capabilities on Mobile Devices
                    </papertitle>
                  <br>
                  <br>
                <paper>
                  Yuedong Yang,
                  <b>Zihui Xue</b>,
                  Radu Marculescu
                </paper>
                  <br>
                  <papervenue>
                  CoRL, 2021
                  </papervenue>
                  <a href="https://openreview.net/forum?id=I6DLxqk9J0A">[paper]</a>
                  <br>
                  <paper>
                    Anytime Depth Estimation with energy-saving 2D LiDARs and monocular cameras.
                  </paper>
                </td>
              </tr>
          </table>

          <!-- <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <b> (c) Network Science </b>
                </td>
              </tr>
            </tbody>
          </table> -->

          <!-- <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/randomwalk.png"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>Sampling Graphlets of Multiplex Networks: A Restricted Random Walk Approach
                    </papertitle>
                  <br>
                  <br>
                  Simiao Jiao,
                  <b>Zihui Xue</b>,
                  Xiaowei Chen,
                  Yuedong Xu

                  <br>
                  <font color="#ca0020">
                  ACM Transactions on the Web (TWEB), 2021
                  </font>
                  <br>
                  <br>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3456291">[paper]</a>
                  <p>
                    A random walk approach to estimate the graphlet concentration in multiplex networks.
                  </p>
                </td>
              </tr>
          </table> -->
